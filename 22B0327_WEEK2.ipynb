{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6/u02Jta1Vn1/SQz+wBqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prarthana-10/GPT-Mastery-SOC/blob/main/22B0327_WEEK2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y5TsjEzgNxU",
        "outputId": "5c8bca6e-a95a-414f-f9ba-b312fe695265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Training Bigram Model\n",
            "Epoch 1, Loss: 2.9311379194259644\n",
            "Epoch 2, Loss: 2.91579532623291\n",
            "Epoch 3, Loss: 2.9003453254699707\n",
            "Epoch 4, Loss: 2.8873504400253296\n",
            "Epoch 5, Loss: 2.872705578804016\n",
            "Epoch 6, Loss: 2.8583253622055054\n",
            "Epoch 7, Loss: 2.8455872535705566\n",
            "Epoch 8, Loss: 2.8331230878829956\n",
            "Epoch 9, Loss: 2.8183681964874268\n",
            "Epoch 10, Loss: 2.8042062520980835\n",
            "Training Trigram Model\n",
            "Epoch 1, Loss: 2.9379009008407593\n",
            "Epoch 2, Loss: 2.9206730127334595\n",
            "Epoch 3, Loss: 2.904275417327881\n",
            "Epoch 4, Loss: 2.8893123865127563\n",
            "Epoch 5, Loss: 2.872227191925049\n",
            "Epoch 6, Loss: 2.8568605184555054\n",
            "Epoch 7, Loss: 2.841080904006958\n",
            "Epoch 8, Loss: 2.8267154693603516\n",
            "Epoch 9, Loss: 2.810738682746887\n",
            "Epoch 10, Loss: 2.7942320108413696\n",
            "Bigram Model Loss: 2.7945468425750732\n",
            "Trigram Model Loss: 2.782409429550171\n"
          ]
        }
      ],
      "source": [
        "# Ensure you have the necessary libraries\n",
        "!pip install torch\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_size=2):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.data = self.create_dataset()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text) - self.context_size):\n",
        "            context = self.text[i:i + self.context_size]\n",
        "            target = self.text[i + self.context_size]\n",
        "            context_idxs = [self.char_to_idx[char] for char in context]\n",
        "            target_idx = self.char_to_idx[target]\n",
        "            data.append((context_idxs, target_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Sample text for training\n",
        "text = \"hello world. this is a simple text for training a trigram model.\"\n",
        "\n",
        "# Create bigram and trigram datasets\n",
        "bigram_dataset = TextDataset(text, context_size=1)\n",
        "trigram_dataset = TextDataset(text, context_size=2)\n",
        "\n",
        "# Create data loaders\n",
        "bigram_dataloader = DataLoader(bigram_dataset, batch_size=32, shuffle=True)\n",
        "trigram_dataloader = DataLoader(trigram_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the bigram model\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().sum(dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the trigram model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(trigram_dataset.vocab)\n",
        "\n",
        "# Instantiate models\n",
        "bigram_model = BigramModel(vocab_size)\n",
        "trigram_model = TrigramModel(vocab_size)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Train the bigram model\n",
        "print(\"Training Bigram Model\")\n",
        "train_model(bigram_model, bigram_dataloader)\n",
        "\n",
        "# Train the trigram model\n",
        "print(\"Training Trigram Model\")\n",
        "train_model(trigram_model, trigram_dataloader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Evaluate the models\n",
        "bigram_loss = evaluate_model(bigram_model, bigram_dataloader)\n",
        "trigram_loss = evaluate_model(trigram_model, trigram_dataloader)\n",
        "\n",
        "print(f\"Bigram Model Loss: {bigram_loss}\")\n",
        "print(f\"Trigram Model Loss: {trigram_loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZnzoCwrMiey-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_size=2):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.data = self.create_dataset()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text) - self.context_size):\n",
        "            context = self.text[i:i + self.context_size]\n",
        "            target = self.text[i + self.context_size]\n",
        "            context_idxs = [self.char_to_idx[char] for char in context]\n",
        "            target_idx = self.char_to_idx[target]\n",
        "            data.append((context_idxs, target_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Sample text for training\n",
        "text = \"hello world. this is a simple text for training a trigram model.\"\n",
        "\n",
        "# Create datasets\n",
        "bigram_dataset = TextDataset(text, context_size=1)\n",
        "trigram_dataset = TextDataset(text, context_size=2)\n",
        "\n",
        "# Split the datasets\n",
        "def split_dataset(dataset, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    dev_size = int(dev_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - dev_size\n",
        "    return random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "bigram_train, bigram_dev, bigram_test = split_dataset(bigram_dataset)\n",
        "trigram_train, trigram_dev, trigram_test = split_dataset(trigram_dataset)\n",
        "\n",
        "# Create data loaders\n",
        "bigram_train_loader = DataLoader(bigram_train, batch_size=32, shuffle=True)\n",
        "bigram_dev_loader = DataLoader(bigram_dev, batch_size=32, shuffle=False)\n",
        "bigram_test_loader = DataLoader(bigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "trigram_train_loader = DataLoader(trigram_train, batch_size=32, shuffle=True)\n",
        "trigram_dev_loader = DataLoader(trigram_dev, batch_size=32, shuffle=False)\n",
        "trigram_test_loader = DataLoader(trigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the bigram model\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().sum(dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the trigram model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(trigram_dataset.vocab)\n",
        "\n",
        "# Instantiate models\n",
        "bigram_model = BigramModel(vocab_size)\n",
        "trigram_model = TrigramModel(vocab_size)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Train the bigram model\n",
        "print(\"Training Bigram Model\")\n",
        "train_model(bigram_model, bigram_train_loader)\n",
        "\n",
        "# Train the trigram model\n",
        "print(\"Training Trigram Model\")\n",
        "train_model(trigram_model, trigram_train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Evaluate the models on dev set\n",
        "bigram_dev_loss = evaluate_model(bigram_model, bigram_dev_loader)\n",
        "trigram_dev_loss = evaluate_model(trigram_model, trigram_dev_loader)\n",
        "\n",
        "# Evaluate the models on test set\n",
        "bigram_test_loss = evaluate_model(bigram_model, bigram_test_loader)\n",
        "trigram_test_loss = evaluate_model(trigram_model, trigram_test_loader)\n",
        "\n",
        "print(f\"Bigram Model Dev Loss: {bigram_dev_loss}\")\n",
        "print(f\"Trigram Model Dev Loss: {trigram_dev_loss}\")\n",
        "print(f\"Bigram Model Test Loss: {bigram_test_loss}\")\n",
        "print(f\"Trigram Model Test Loss: {trigram_test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY_qdY8ii4pZ",
        "outputId": "a9539719-d685-418d-aac6-dc16d1004250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bigram Model\n",
            "Epoch 1, Loss: 2.968363881111145\n",
            "Epoch 2, Loss: 2.9524093866348267\n",
            "Epoch 3, Loss: 2.9329882860183716\n",
            "Epoch 4, Loss: 2.9222463369369507\n",
            "Epoch 5, Loss: 2.9082382917404175\n",
            "Epoch 6, Loss: 2.897231698036194\n",
            "Epoch 7, Loss: 2.8811813592910767\n",
            "Epoch 8, Loss: 2.876084089279175\n",
            "Epoch 9, Loss: 2.8507518768310547\n",
            "Epoch 10, Loss: 2.845628261566162\n",
            "Training Trigram Model\n",
            "Epoch 1, Loss: 2.9443992376327515\n",
            "Epoch 2, Loss: 2.9195446968078613\n",
            "Epoch 3, Loss: 2.9103747606277466\n",
            "Epoch 4, Loss: 2.897378087043762\n",
            "Epoch 5, Loss: 2.877183198928833\n",
            "Epoch 6, Loss: 2.859540343284607\n",
            "Epoch 7, Loss: 2.8442124128341675\n",
            "Epoch 8, Loss: 2.833723306655884\n",
            "Epoch 9, Loss: 2.8084535598754883\n",
            "Epoch 10, Loss: 2.793848156929016\n",
            "Bigram Model Dev Loss: 2.879796028137207\n",
            "Trigram Model Dev Loss: 2.865471124649048\n",
            "Bigram Model Test Loss: 2.833972215652466\n",
            "Trigram Model Test Loss: 2.9337661266326904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_size=2):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.data = self.create_dataset()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text) - self.context_size):\n",
        "            context = self.text[i:i + self.context_size]\n",
        "            target = self.text[i + self.context_size]\n",
        "            context_idxs = [self.char_to_idx[char] for char in context]\n",
        "            target_idx = self.char_to_idx[target]\n",
        "            data.append((context_idxs, target_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Sample text for training\n",
        "text = \"hello world. this is a simple text for training a trigram model.\"\n",
        "\n",
        "# Create datasets\n",
        "bigram_dataset = TextDataset(text, context_size=1)\n",
        "trigram_dataset = TextDataset(text, context_size=2)\n",
        "\n",
        "# Split the datasets\n",
        "def split_dataset(dataset, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    dev_size = int(dev_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - dev_size\n",
        "    return random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "bigram_train, bigram_dev, bigram_test = split_dataset(bigram_dataset)\n",
        "trigram_train, trigram_dev, trigram_test = split_dataset(trigram_dataset)\n",
        "\n",
        "# Create data loaders\n",
        "bigram_train_loader = DataLoader(bigram_train, batch_size=32, shuffle=True)\n",
        "bigram_dev_loader = DataLoader(bigram_dev, batch_size=32, shuffle=False)\n",
        "bigram_test_loader = DataLoader(bigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "trigram_train_loader = DataLoader(trigram_train, batch_size=32, shuffle=True)\n",
        "trigram_dev_loader = DataLoader(trigram_dev, batch_size=32, shuffle=False)\n",
        "trigram_test_loader = DataLoader(trigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the bigram model\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().sum(dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the trigram model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float().view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(trigram_dataset.vocab)\n",
        "\n",
        "# Instantiate models\n",
        "bigram_model = BigramModel(vocab_size)\n",
        "trigram_model = TrigramModel(vocab_size)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=10, weight_decay=0.0):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Train the bigram model without regularization\n",
        "print(\"Training Bigram Model\")\n",
        "train_model(bigram_model, bigram_train_loader)\n",
        "\n",
        "# Evaluate the bigram model on dev and test sets\n",
        "bigram_dev_loss = evaluate_model(bigram_model, bigram_dev_loader)\n",
        "bigram_test_loss = evaluate_model(bigram_model, bigram_test_loader)\n",
        "\n",
        "print(f\"Bigram Model Dev Loss: {bigram_dev_loss}\")\n",
        "print(f\"Bigram Model Test Loss: {bigram_test_loss}\")\n",
        "\n",
        "# Tune the regularization strength for the trigram model\n",
        "weight_decays = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
        "best_weight_decay = 0.0\n",
        "best_dev_loss = float('inf')\n",
        "\n",
        "for weight_decay in weight_decays:\n",
        "    print(f\"\\nTraining Trigram Model with weight_decay={weight_decay}\")\n",
        "    model = TrigramModel(vocab_size)\n",
        "    train_model(model, trigram_train_loader, weight_decay=weight_decay)\n",
        "    dev_loss = evaluate_model(model, trigram_dev_loader)\n",
        "    print(f\"Dev Loss: {dev_loss}\")\n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        best_weight_decay = weight_decay\n",
        "\n",
        "print(f\"\\nBest weight decay: {best_weight_decay}, Dev Loss: {best_dev_loss}\")\n",
        "\n",
        "# Evaluate the best trigram model on test set\n",
        "best_trigram_model = TrigramModel(vocab_size)\n",
        "train_model(best_trigram_model, trigram_train_loader, weight_decay=best_weight_decay)\n",
        "trigram_test_loss = evaluate_model(best_trigram_model, trigram_test_loader)\n",
        "\n",
        "print(f\"Trigram Model Test Loss with best weight decay ({best_weight_decay}): {trigram_test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UANVkXkji89a",
        "outputId": "a8be01f6-f0ea-4774-c381-8fd2d8938c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bigram Model\n",
            "Epoch 1, Loss: 2.9531177282333374\n",
            "Epoch 2, Loss: 2.935624122619629\n",
            "Epoch 3, Loss: 2.915940284729004\n",
            "Epoch 4, Loss: 2.8986111879348755\n",
            "Epoch 5, Loss: 2.888336658477783\n",
            "Epoch 6, Loss: 2.8679792881011963\n",
            "Epoch 7, Loss: 2.857132315635681\n",
            "Epoch 8, Loss: 2.8377009630203247\n",
            "Epoch 9, Loss: 2.8311187028884888\n",
            "Epoch 10, Loss: 2.8222053050994873\n",
            "Bigram Model Dev Loss: 2.951111078262329\n",
            "Bigram Model Test Loss: 2.9269378185272217\n",
            "\n",
            "Training Trigram Model with weight_decay=0.0\n",
            "Epoch 1, Loss: 2.9539849758148193\n",
            "Epoch 2, Loss: 2.926152467727661\n",
            "Epoch 3, Loss: 2.9019569158554077\n",
            "Epoch 4, Loss: 2.900794267654419\n",
            "Epoch 5, Loss: 2.8654823303222656\n",
            "Epoch 6, Loss: 2.8587454557418823\n",
            "Epoch 7, Loss: 2.8359209299087524\n",
            "Epoch 8, Loss: 2.814158320426941\n",
            "Epoch 9, Loss: 2.8017191886901855\n",
            "Epoch 10, Loss: 2.771131753921509\n",
            "Dev Loss: 2.8956024646759033\n",
            "\n",
            "Training Trigram Model with weight_decay=0.001\n",
            "Epoch 1, Loss: 2.9386545419692993\n",
            "Epoch 2, Loss: 2.919864296913147\n",
            "Epoch 3, Loss: 2.9063303470611572\n",
            "Epoch 4, Loss: 2.8828413486480713\n",
            "Epoch 5, Loss: 2.8664296865463257\n",
            "Epoch 6, Loss: 2.8528987169265747\n",
            "Epoch 7, Loss: 2.839336633682251\n",
            "Epoch 8, Loss: 2.8221354484558105\n",
            "Epoch 9, Loss: 2.801563262939453\n",
            "Epoch 10, Loss: 2.785369038581848\n",
            "Dev Loss: 2.899338722229004\n",
            "\n",
            "Training Trigram Model with weight_decay=0.01\n",
            "Epoch 1, Loss: 2.947719931602478\n",
            "Epoch 2, Loss: 2.9286811351776123\n",
            "Epoch 3, Loss: 2.9111828804016113\n",
            "Epoch 4, Loss: 2.9002751111984253\n",
            "Epoch 5, Loss: 2.881954550743103\n",
            "Epoch 6, Loss: 2.8724805116653442\n",
            "Epoch 7, Loss: 2.8523823022842407\n",
            "Epoch 8, Loss: 2.835834503173828\n",
            "Epoch 9, Loss: 2.821588397026062\n",
            "Epoch 10, Loss: 2.813106656074524\n",
            "Dev Loss: 2.922811269760132\n",
            "\n",
            "Training Trigram Model with weight_decay=0.1\n",
            "Epoch 1, Loss: 2.9341838359832764\n",
            "Epoch 2, Loss: 2.929790735244751\n",
            "Epoch 3, Loss: 2.918049454689026\n",
            "Epoch 4, Loss: 2.907987594604492\n",
            "Epoch 5, Loss: 2.895018696784973\n",
            "Epoch 6, Loss: 2.895327091217041\n",
            "Epoch 7, Loss: 2.8798717260360718\n",
            "Epoch 8, Loss: 2.8745328187942505\n",
            "Epoch 9, Loss: 2.8725178241729736\n",
            "Epoch 10, Loss: 2.8602046966552734\n",
            "Dev Loss: 2.95234751701355\n",
            "\n",
            "Training Trigram Model with weight_decay=1.0\n",
            "Epoch 1, Loss: 2.9427486658096313\n",
            "Epoch 2, Loss: 2.9492952823638916\n",
            "Epoch 3, Loss: 2.946104884147644\n",
            "Epoch 4, Loss: 2.942564010620117\n",
            "Epoch 5, Loss: 2.939786195755005\n",
            "Epoch 6, Loss: 2.938283085823059\n",
            "Epoch 7, Loss: 2.936662435531616\n",
            "Epoch 8, Loss: 2.937659740447998\n",
            "Epoch 9, Loss: 2.9353582859039307\n",
            "Epoch 10, Loss: 2.9340415000915527\n",
            "Dev Loss: 2.958272695541382\n",
            "\n",
            "Best weight decay: 0.0, Dev Loss: 2.8956024646759033\n",
            "Epoch 1, Loss: 2.9567196369171143\n",
            "Epoch 2, Loss: 2.938158869743347\n",
            "Epoch 3, Loss: 2.9135444164276123\n",
            "Epoch 4, Loss: 2.8930957317352295\n",
            "Epoch 5, Loss: 2.874383568763733\n",
            "Epoch 6, Loss: 2.856600761413574\n",
            "Epoch 7, Loss: 2.8416473865509033\n",
            "Epoch 8, Loss: 2.8271814584732056\n",
            "Epoch 9, Loss: 2.800699234008789\n",
            "Epoch 10, Loss: 2.7981680631637573\n",
            "Trigram Model Test Loss with best weight decay (0.0): 2.8949477672576904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_size=2):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.data = self.create_dataset()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text) - self.context_size):\n",
        "            context = self.text[i:i + self.context_size]\n",
        "            target = self.text[i + self.context_size]\n",
        "            context_idxs = [self.char_to_idx[char] for char in context]\n",
        "            target_idx = self.char_to_idx[target]\n",
        "            data.append((context_idxs, target_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Sample text for training\n",
        "text = \"hello world. this is a simple text for training a trigram model.\"\n",
        "\n",
        "# Create datasets\n",
        "bigram_dataset = TextDataset(text, context_size=1)\n",
        "trigram_dataset = TextDataset(text, context_size=2)\n",
        "\n",
        "# Split the datasets\n",
        "def split_dataset(dataset, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    dev_size = int(dev_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - dev_size\n",
        "    return random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "bigram_train, bigram_dev, bigram_test = split_dataset(bigram_dataset)\n",
        "trigram_train, trigram_dev, trigram_test = split_dataset(trigram_dataset)\n",
        "\n",
        "# Create data loaders\n",
        "bigram_train_loader = DataLoader(bigram_train, batch_size=32, shuffle=True)\n",
        "bigram_dev_loader = DataLoader(bigram_dev, batch_size=32, shuffle=False)\n",
        "bigram_test_loader = DataLoader(bigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "trigram_train_loader = DataLoader(trigram_train, batch_size=32, shuffle=True)\n",
        "trigram_dev_loader = DataLoader(trigram_dev, batch_size=32, shuffle=False)\n",
        "trigram_test_loader = DataLoader(trigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the bigram model\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).sum(dim=1)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the trigram model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128 * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(trigram_dataset.vocab)\n",
        "\n",
        "# Instantiate models\n",
        "bigram_model = BigramModel(vocab_size)\n",
        "trigram_model = TrigramModel(vocab_size)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=10, weight_decay=0.0):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Train the bigram model without regularization\n",
        "print(\"Training Bigram Model\")\n",
        "train_model(bigram_model, bigram_train_loader)\n",
        "\n",
        "# Evaluate the bigram model on dev and test sets\n",
        "bigram_dev_loss = evaluate_model(bigram_model, bigram_dev_loader)\n",
        "bigram_test_loss = evaluate_model(bigram_model, bigram_test_loader)\n",
        "\n",
        "print(f\"Bigram Model Dev Loss: {bigram_dev_loss}\")\n",
        "print(f\"Bigram Model Test Loss: {bigram_test_loss}\")\n",
        "\n",
        "# Tune the regularization strength for the trigram model\n",
        "weight_decays = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
        "best_weight_decay = 0.0\n",
        "best_dev_loss = float('inf')\n",
        "\n",
        "for weight_decay in weight_decays:\n",
        "    print(f\"\\nTraining Trigram Model with weight_decay={weight_decay}\")\n",
        "    model = TrigramModel(vocab_size)\n",
        "    train_model(model, trigram_train_loader, weight_decay=weight_decay)\n",
        "    dev_loss = evaluate_model(model, trigram_dev_loader)\n",
        "    print(f\"Dev Loss: {dev_loss}\")\n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        best_weight_decay = weight_decay\n",
        "\n",
        "print(f\"\\nBest weight decay: {best_weight_decay}, Dev Loss: {best_dev_loss}\")\n",
        "\n",
        "# Evaluate the best trigram model on test set\n",
        "best_trigram_model = TrigramModel(vocab_size)\n",
        "train_model(best_trigram_model, trigram_train_loader, weight_decay=best_weight_decay)\n",
        "trigram_test_loss = evaluate_model(best_trigram_model, trigram_test_loader)\n",
        "\n",
        "print(f\"Trigram Model Test Loss with best weight decay ({best_weight_decay}): {trigram_test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5HyvD2ljeF-",
        "outputId": "405e36ed-f0f2-425e-e1ed-b55e2156c8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bigram Model\n",
            "Epoch 1, Loss: 3.1371724605560303\n",
            "Epoch 2, Loss: 3.0789023637771606\n",
            "Epoch 3, Loss: 2.9869070053100586\n",
            "Epoch 4, Loss: 2.908730983734131\n",
            "Epoch 5, Loss: 2.8277335166931152\n",
            "Epoch 6, Loss: 2.7639886140823364\n",
            "Epoch 7, Loss: 2.6885894536972046\n",
            "Epoch 8, Loss: 2.6682519912719727\n",
            "Epoch 9, Loss: 2.6143622398376465\n",
            "Epoch 10, Loss: 2.522128939628601\n",
            "Bigram Model Dev Loss: 2.8781604766845703\n",
            "Bigram Model Test Loss: 3.0224175453186035\n",
            "\n",
            "Training Trigram Model with weight_decay=0.0\n",
            "Epoch 1, Loss: 3.1593868732452393\n",
            "Epoch 2, Loss: 2.9626407623291016\n",
            "Epoch 3, Loss: 2.800540566444397\n",
            "Epoch 4, Loss: 2.662756085395813\n",
            "Epoch 5, Loss: 2.527352452278137\n",
            "Epoch 6, Loss: 2.383245825767517\n",
            "Epoch 7, Loss: 2.2904163599014282\n",
            "Epoch 8, Loss: 2.186179280281067\n",
            "Epoch 9, Loss: 2.084089159965515\n",
            "Epoch 10, Loss: 1.9861007332801819\n",
            "Dev Loss: 3.4883201122283936\n",
            "\n",
            "Training Trigram Model with weight_decay=0.001\n",
            "Epoch 1, Loss: 2.969280242919922\n",
            "Epoch 2, Loss: 2.78306245803833\n",
            "Epoch 3, Loss: 2.599886417388916\n",
            "Epoch 4, Loss: 2.5324881076812744\n",
            "Epoch 5, Loss: 2.3526623249053955\n",
            "Epoch 6, Loss: 2.2713570594787598\n",
            "Epoch 7, Loss: 2.2220557928085327\n",
            "Epoch 8, Loss: 2.1490731239318848\n",
            "Epoch 9, Loss: 1.9800370931625366\n",
            "Epoch 10, Loss: 1.9590014219284058\n",
            "Dev Loss: 3.79805064201355\n",
            "\n",
            "Training Trigram Model with weight_decay=0.01\n",
            "Epoch 1, Loss: 3.062906503677368\n",
            "Epoch 2, Loss: 2.8672081232070923\n",
            "Epoch 3, Loss: 2.7185062170028687\n",
            "Epoch 4, Loss: 2.6020259857177734\n",
            "Epoch 5, Loss: 2.4827725887298584\n",
            "Epoch 6, Loss: 2.3639638423919678\n",
            "Epoch 7, Loss: 2.237015128135681\n",
            "Epoch 8, Loss: 2.1767746210098267\n",
            "Epoch 9, Loss: 2.111909866333008\n",
            "Epoch 10, Loss: 1.9876137375831604\n",
            "Dev Loss: 3.678210973739624\n",
            "\n",
            "Training Trigram Model with weight_decay=0.1\n",
            "Epoch 1, Loss: 2.932968497276306\n",
            "Epoch 2, Loss: 2.778580904006958\n",
            "Epoch 3, Loss: 2.6518843173980713\n",
            "Epoch 4, Loss: 2.551101326942444\n",
            "Epoch 5, Loss: 2.403146505355835\n",
            "Epoch 6, Loss: 2.3297173976898193\n",
            "Epoch 7, Loss: 2.160435140132904\n",
            "Epoch 8, Loss: 2.139904737472534\n",
            "Epoch 9, Loss: 2.0884827375411987\n",
            "Epoch 10, Loss: 1.935018002986908\n",
            "Dev Loss: 3.6393611431121826\n",
            "\n",
            "Training Trigram Model with weight_decay=1.0\n",
            "Epoch 1, Loss: 2.9816261529922485\n",
            "Epoch 2, Loss: 2.881405234336853\n",
            "Epoch 3, Loss: 2.7752009630203247\n",
            "Epoch 4, Loss: 2.7169629335403442\n",
            "Epoch 5, Loss: 2.665367007255554\n",
            "Epoch 6, Loss: 2.6037511825561523\n",
            "Epoch 7, Loss: 2.5188640356063843\n",
            "Epoch 8, Loss: 2.4325194358825684\n",
            "Epoch 9, Loss: 2.4184621572494507\n",
            "Epoch 10, Loss: 2.3447928428649902\n",
            "Dev Loss: 3.272089719772339\n",
            "\n",
            "Best weight decay: 1.0, Dev Loss: 3.272089719772339\n",
            "Epoch 1, Loss: 2.972036123275757\n",
            "Epoch 2, Loss: 2.8838526010513306\n",
            "Epoch 3, Loss: 2.802331805229187\n",
            "Epoch 4, Loss: 2.6991019248962402\n",
            "Epoch 5, Loss: 2.5971518754959106\n",
            "Epoch 6, Loss: 2.553813099861145\n",
            "Epoch 7, Loss: 2.4867215156555176\n",
            "Epoch 8, Loss: 2.4358315467834473\n",
            "Epoch 9, Loss: 2.3672465085983276\n",
            "Epoch 10, Loss: 2.332282781600952\n",
            "Trigram Model Test Loss with best weight decay (1.0): 2.9992592334747314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Define the dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_size=2):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.data = self.create_dataset()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text) - self.context_size):\n",
        "            context = self.text[i:i + self.context_size]\n",
        "            target = self.text[i + self.context_size]\n",
        "            context_idxs = [self.char_to_idx[char] for char in context]\n",
        "            target_idx = self.char_to_idx[target]\n",
        "            data.append((context_idxs, target_idx))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "# Sample text for training\n",
        "text = \"hello world. this is a simple text for training a trigram model.\"\n",
        "\n",
        "# Create datasets\n",
        "bigram_dataset = TextDataset(text, context_size=1)\n",
        "trigram_dataset = TextDataset(text, context_size=2)\n",
        "\n",
        "# Split the datasets\n",
        "def split_dataset(dataset, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    dev_size = int(dev_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - dev_size\n",
        "    return random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "bigram_train, bigram_dev, bigram_test = split_dataset(bigram_dataset)\n",
        "trigram_train, trigram_dev, trigram_test = split_dataset(trigram_dataset)\n",
        "\n",
        "# Create data loaders\n",
        "bigram_train_loader = DataLoader(bigram_train, batch_size=32, shuffle=True)\n",
        "bigram_dev_loader = DataLoader(bigram_dev, batch_size=32, shuffle=False)\n",
        "bigram_test_loader = DataLoader(bigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "trigram_train_loader = DataLoader(trigram_train, batch_size=32, shuffle=True)\n",
        "trigram_dev_loader = DataLoader(trigram_dev, batch_size=32, shuffle=False)\n",
        "trigram_test_loader = DataLoader(trigram_test, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the bigram model\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).sum(dim=1)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the trigram model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, 128)\n",
        "        self.fc2 = nn.Linear(128 * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(trigram_dataset.vocab)\n",
        "\n",
        "# Instantiate models\n",
        "bigram_model = BigramModel(vocab_size)\n",
        "trigram_model = TrigramModel(vocab_size)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=10, weight_decay=0.0):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Train the bigram model without regularization\n",
        "print(\"Training Bigram Model\")\n",
        "train_model(bigram_model, bigram_train_loader)\n",
        "\n",
        "# Evaluate the bigram model on dev and test sets\n",
        "bigram_dev_loss = evaluate_model(bigram_model, bigram_dev_loader)\n",
        "bigram_test_loss = evaluate_model(bigram_model, bigram_test_loader)\n",
        "\n",
        "print(f\"Bigram Model Dev Loss: {bigram_dev_loss}\")\n",
        "print(f\"Bigram Model Test Loss: {bigram_test_loss}\")\n",
        "\n",
        "# Tune the regularization strength for the trigram model\n",
        "weight_decays = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
        "best_weight_decay = 0.0\n",
        "best_dev_loss = float('inf')\n",
        "\n",
        "for weight_decay in weight_decays:\n",
        "    print(f\"\\nTraining Trigram Model with weight_decay={weight_decay}\")\n",
        "    model = TrigramModel(vocab_size)\n",
        "    train_model(model, trigram_train_loader, weight_decay=weight_decay)\n",
        "    dev_loss = evaluate_model(model, trigram_dev_loader)\n",
        "    print(f\"Dev Loss: {dev_loss}\")\n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        best_weight_decay = weight_decay\n",
        "\n",
        "print(f\"\\nBest weight decay: {best_weight_decay}, Dev Loss: {best_dev_loss}\")\n",
        "\n",
        "# Evaluate the best trigram model on test set\n",
        "best_trigram_model = TrigramModel(vocab_size)\n",
        "train_model(best_trigram_model, trigram_train_loader, weight_decay=best_weight_decay)\n",
        "trigram_test_loss = evaluate_model(best_trigram_model, trigram_test_loader)\n",
        "\n",
        "print(f\"Trigram Model Test Loss with best weight decay ({best_weight_decay}): {trigram_test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CLu_7HUj_xh",
        "outputId": "4ba07926-f2f6-4bf3-ea8a-360e8290a457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bigram Model\n",
            "Epoch 1, Loss: 2.8900279998779297\n",
            "Epoch 2, Loss: 2.8204110860824585\n",
            "Epoch 3, Loss: 2.74286425113678\n",
            "Epoch 4, Loss: 2.6939122676849365\n",
            "Epoch 5, Loss: 2.5935546159744263\n",
            "Epoch 6, Loss: 2.5574045181274414\n",
            "Epoch 7, Loss: 2.4643744230270386\n",
            "Epoch 8, Loss: 2.4315407276153564\n",
            "Epoch 9, Loss: 2.335947036743164\n",
            "Epoch 10, Loss: 2.3249012231826782\n",
            "Bigram Model Dev Loss: 3.0295238494873047\n",
            "Bigram Model Test Loss: 3.198779582977295\n",
            "\n",
            "Training Trigram Model with weight_decay=0.0\n",
            "Epoch 1, Loss: 2.9970760345458984\n",
            "Epoch 2, Loss: 2.837743878364563\n",
            "Epoch 3, Loss: 2.7239545583724976\n",
            "Epoch 4, Loss: 2.6064172983169556\n",
            "Epoch 5, Loss: 2.4771848917007446\n",
            "Epoch 6, Loss: 2.399138331413269\n",
            "Epoch 7, Loss: 2.3023072481155396\n",
            "Epoch 8, Loss: 2.219529628753662\n",
            "Epoch 9, Loss: 2.163233757019043\n",
            "Epoch 10, Loss: 2.0930097103118896\n",
            "Dev Loss: 2.2652432918548584\n",
            "\n",
            "Training Trigram Model with weight_decay=0.001\n",
            "Epoch 1, Loss: 3.1107257604599\n",
            "Epoch 2, Loss: 2.9711945056915283\n",
            "Epoch 3, Loss: 2.8356857299804688\n",
            "Epoch 4, Loss: 2.730186939239502\n",
            "Epoch 5, Loss: 2.616843104362488\n",
            "Epoch 6, Loss: 2.528349757194519\n",
            "Epoch 7, Loss: 2.436041235923767\n",
            "Epoch 8, Loss: 2.336642861366272\n",
            "Epoch 9, Loss: 2.2397814989089966\n",
            "Epoch 10, Loss: 2.170749306678772\n",
            "Dev Loss: 2.2914812564849854\n",
            "\n",
            "Training Trigram Model with weight_decay=0.01\n",
            "Epoch 1, Loss: 3.0099326372146606\n",
            "Epoch 2, Loss: 2.845829963684082\n",
            "Epoch 3, Loss: 2.6890686750411987\n",
            "Epoch 4, Loss: 2.5958467721939087\n",
            "Epoch 5, Loss: 2.50363028049469\n",
            "Epoch 6, Loss: 2.4334722757339478\n",
            "Epoch 7, Loss: 2.323165774345398\n",
            "Epoch 8, Loss: 2.2832190990448\n",
            "Epoch 9, Loss: 2.1654528379440308\n",
            "Epoch 10, Loss: 2.105754256248474\n",
            "Dev Loss: 2.114246368408203\n",
            "\n",
            "Training Trigram Model with weight_decay=0.1\n",
            "Epoch 1, Loss: 2.992792844772339\n",
            "Epoch 2, Loss: 2.850863218307495\n",
            "Epoch 3, Loss: 2.696645498275757\n",
            "Epoch 4, Loss: 2.594303011894226\n",
            "Epoch 5, Loss: 2.4769349098205566\n",
            "Epoch 6, Loss: 2.3443506956100464\n",
            "Epoch 7, Loss: 2.2892632484436035\n",
            "Epoch 8, Loss: 2.19238018989563\n",
            "Epoch 9, Loss: 2.14155650138855\n",
            "Epoch 10, Loss: 2.0301389694213867\n",
            "Dev Loss: 2.454272985458374\n",
            "\n",
            "Training Trigram Model with weight_decay=1.0\n",
            "Epoch 1, Loss: 2.9496819972991943\n",
            "Epoch 2, Loss: 2.88042151927948\n",
            "Epoch 3, Loss: 2.7932690382003784\n",
            "Epoch 4, Loss: 2.7710719108581543\n",
            "Epoch 5, Loss: 2.704895853996277\n",
            "Epoch 6, Loss: 2.620961546897888\n",
            "Epoch 7, Loss: 2.588939666748047\n",
            "Epoch 8, Loss: 2.5354923009872437\n",
            "Epoch 9, Loss: 2.5111498832702637\n",
            "Epoch 10, Loss: 2.4912444353103638\n",
            "Dev Loss: 2.429940938949585\n",
            "\n",
            "Best weight decay: 0.01, Dev Loss: 2.114246368408203\n",
            "Epoch 1, Loss: 3.1219300031661987\n",
            "Epoch 2, Loss: 2.9086586236953735\n",
            "Epoch 3, Loss: 2.8081682920455933\n",
            "Epoch 4, Loss: 2.6669373512268066\n",
            "Epoch 5, Loss: 2.5880956649780273\n",
            "Epoch 6, Loss: 2.5010653734207153\n",
            "Epoch 7, Loss: 2.4088300466537476\n",
            "Epoch 8, Loss: 2.318890333175659\n",
            "Epoch 9, Loss: 2.2399603128433228\n",
            "Epoch 10, Loss: 2.1507354974746704\n",
            "Trigram Model Test Loss with best weight decay (0.01): 2.5352895259857178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNyXm1uikc19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}