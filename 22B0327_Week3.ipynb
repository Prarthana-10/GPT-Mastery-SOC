{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4i+eswsInd2R6JP7AwGf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prarthana-10/GPT-Mastery-SOC/blob/main/22B0327_Week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st ques"
      ],
      "metadata": {
        "id": "LCNZaNGob1m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "# Assume we have 10 classes (e.g., MNIST dataset)\n",
        "num_classes = 10\n",
        "\n",
        "# Create uniform predictions\n",
        "uniform_predictions = np.full((num_classes,), 1 / num_classes)\n",
        "\n",
        "# Create dummy labels (true labels for a batch of 100 samples, for example)\n",
        "true_labels = np.random.randint(0, num_classes, size=(100,))\n",
        "\n",
        "# Convert true labels to one-hot encoded format\n",
        "one_hot_labels = tf.one_hot(true_labels, depth=num_classes)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "\n",
        "# Calculate the loss for uniform predictions\n",
        "uniform_loss = loss_fn(one_hot_labels, np.tile(uniform_predictions, (100, 1)))\n",
        "print(f\"Loss for uniform predictions: {uniform_loss.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1M2jrJcXm7h",
        "outputId": "8086c328-d4e1-4222-fb95-5863481a2f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for uniform predictions: 2.302585092994045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd ques"
      ],
      "metadata": {
        "id": "zNFFSVX_b3ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "\n",
        "# Define the model\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),  # Assuming MNIST input shape\n",
        "        layers.Dense(10, activation='softmax', kernel_initializer=initializers.Zeros())\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create dummy data (e.g., MNIST-like data)\n",
        "x_dummy = np.random.rand(100, 28, 28)\n",
        "y_dummy = np.random.randint(0, 10, 100)\n",
        "\n",
        "# Evaluate the model before training to get the initial loss\n",
        "initial_loss, _ = model.evaluate(x_dummy, y_dummy, verbose=0)\n",
        "print(f\"Initial loss: {initial_loss}\")\n",
        "\n",
        "# Now, let's train the model for one epoch to check if the loss improves\n",
        "model.fit(x_dummy, y_dummy, epochs=1, verbose=2)\n",
        "\n",
        "# Evaluate the model after one epoch of training\n",
        "trained_loss, _ = model.evaluate(x_dummy, y_dummy, verbose=0)\n",
        "print(f\"Loss after one epoch of training: {trained_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln7OgNxgXpDW",
        "outputId": "1d85cb44-49dc-42ec-f69d-23c581c4b6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 2.3025851249694824\n",
            "4/4 - 0s - loss: 2.3338 - accuracy: 0.1100 - 377ms/epoch - 94ms/step\n",
            "Loss after one epoch of training: 2.1911497116088867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Training Using Autoencoders\n",
        "We'll use the MNIST dataset for this example."
      ],
      "metadata": {
        "id": "xX69UaTuYBva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((x_train.shape[0], -1))  # Flatten the images\n",
        "x_test = x_test.reshape((x_test.shape[0], -1))  # Flatten the images\n",
        "\n",
        "# Define autoencoder model\n",
        "def build_autoencoder(input_dim, encoding_dim):\n",
        "    input_layer = layers.Input(shape=(input_dim,))\n",
        "    encoded = layers.Dense(encoding_dim, activation='relu', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros())(input_layer)\n",
        "    decoded = layers.Dense(input_dim, activation='sigmoid', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros())(encoded)\n",
        "    autoencoder = models.Model(inputs=input_layer, outputs=decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return autoencoder\n",
        "\n",
        "# Train autoencoders layer by layer\n",
        "encoding_dims = [128, 64, 32]\n",
        "autoencoders = []\n",
        "input_dim = x_train.shape[1]\n",
        "\n",
        "for encoding_dim in encoding_dims:\n",
        "    autoencoder = build_autoencoder(input_dim, encoding_dim)\n",
        "    print(f\"Training autoencoder with input_dim={input_dim} and encoding_dim={encoding_dim}\")\n",
        "    autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
        "    autoencoders.append(autoencoder)\n",
        "    # Update input_dim for the next autoencoder\n",
        "    input_dim = encoding_dim\n",
        "\n",
        "# Create a model to encode data using the trained autoencoders\n",
        "def encode_data(autoencoders, data):\n",
        "    for autoencoder in autoencoders:\n",
        "        data = autoencoder.predict(data)\n",
        "    return data\n",
        "\n",
        "# Encode training and test data\n",
        "x_train_encoded = encode_data(autoencoders, x_train)\n",
        "x_test_encoded = encode_data(autoencoders, x_test)\n",
        "\n",
        "# Define the supervised model\n",
        "def build_supervised_model(input_dim):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros()),\n",
        "        layers.Dense(64, activation='relu', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros()),\n",
        "        layers.Dense(10, activation='softmax', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros())\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and train the supervised model\n",
        "supervised_model = build_supervised_model(encoding_dims[-1])\n",
        "supervised_model.fit(x_train_encoded, y_train, epochs=10, batch_size=256, validation_data=(x_test_encoded, y_test))\n",
        "\n",
        "# Evaluate the supervised model\n",
        "test_loss, test_acc = supervised_model.evaluate(x_test_encoded, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "# Inspect gradients and activations\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Function to get gradients\n",
        "def get_gradients(model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = model.compiled_loss(y, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    return gradients\n",
        "\n",
        "# Function to get activations\n",
        "def get_activations(model, x):\n",
        "    activations = []\n",
        "    for layer in model.layers:\n",
        "        x = layer(x)\n",
        "        activations.append(x)\n",
        "    return activations\n",
        "\n",
        "# Get gradients and activations for a batch of data\n",
        "batch_x = x_train_encoded[:10]\n",
        "batch_y = y_train[:10]\n",
        "gradients = get_gradients(supervised_model, batch_x, batch_y)\n",
        "activations = get_activations(supervised_model, batch_x)\n",
        "\n",
        "# Print shapes of gradients and activations\n",
        "for i, grad in enumerate(gradients):\n",
        "    print(f\"Gradient {i}: {grad.shape}\")\n",
        "\n",
        "for i, act in enumerate(activations):\n",
        "    print(f\"Activation {i}: {act.shape}\")\n",
        "\n",
        "# Check the loss values from history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = supervised_model.history\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c7smj5cDYDF2",
        "outputId": "5435cf5b-e43e-4afb-a14a-62d582d0df5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training autoencoder with input_dim=784 and encoding_dim=128\n",
            "Epoch 1/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.6530 - val_loss: 0.6156\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5833 - val_loss: 0.5535\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.5275 - val_loss: 0.5038\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.4827 - val_loss: 0.4638\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.4467 - val_loss: 0.4314\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.4174 - val_loss: 0.4050\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 3s 15ms/step - loss: 0.3935 - val_loss: 0.3834\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 3s 15ms/step - loss: 0.3739 - val_loss: 0.3656\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.3577 - val_loss: 0.3508\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.3442 - val_loss: 0.3385\n",
            "Training autoencoder with input_dim=128 and encoding_dim=64\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_9\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 784)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-40eae96e27ea>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training autoencoder with input_dim={input_dim} and encoding_dim={encoding_dim}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mautoencoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Update input_dim for the next autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_9\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4th ques"
      ],
      "metadata": {
        "id": "XQp0dWrhalHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((x_train.shape[0], -1))\n",
        "x_test = x_test.reshape((x_test.shape[0], -1))\n",
        "\n",
        "# Define a model with zero initialization\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros(), input_shape=(784,)),\n",
        "        layers.Dense(64, activation='relu', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros()),\n",
        "        layers.Dense(10, activation='softmax', kernel_initializer=initializers.Zeros(), bias_initializer=initializers.Zeros())\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and train the model\n",
        "model = build_model()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=256, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "# Function to get gradients\n",
        "def get_gradients(model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = model.compiled_loss(y, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    return gradients\n",
        "\n",
        "# Create a model to extract activations\n",
        "class ActivationModel(tf.keras.models.Model):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layer_outputs = [layer.output for layer in model.layers]\n",
        "        self.activations_model = tf.keras.models.Model(inputs=model.input, outputs=self.layer_outputs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activations_model(inputs)\n",
        "\n",
        "# Get activations\n",
        "activation_model = ActivationModel(model)\n",
        "def get_activations(model, x):\n",
        "    activations = activation_model(x)\n",
        "    return activations\n",
        "\n",
        "# Get gradients and activations for a batch of data\n",
        "batch_x = x_train[:10]\n",
        "batch_y = y_train[:10]\n",
        "gradients = get_gradients(model, batch_x, batch_y)\n",
        "activations = get_activations(model, batch_x)\n",
        "\n",
        "# Print shapes of gradients and activations\n",
        "for i, grad in enumerate(gradients):\n",
        "    print(f\"Gradient {i}: {grad.shape}\")\n",
        "\n",
        "for i, act in enumerate(activations):\n",
        "    print(f\"Activation {i}: {act.shape}\")\n",
        "\n",
        "# Plot loss values from history\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "5dKkvbutamik",
        "outputId": "64329044-394d-4802-ccb2-55c259b809ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 3s 8ms/step - loss: 2.3018 - accuracy: 0.1119 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3011 - accuracy: 0.1135\n",
            "Test accuracy: 0.11349999904632568\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'rank'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c40dcb6c23d7>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c40dcb6c23d7>\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\u001b[0m in \u001b[0;36mmatch_dtype_and_rank\u001b[0;34m(y_t, y_p, sw)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;34m\"\"\"Match dtype and rank of predictions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5th ques"
      ],
      "metadata": {
        "id": "0ROgFR4abQgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((x_train.shape[0], -1))\n",
        "x_test = x_test.reshape((x_test.shape[0], -1))\n",
        "\n",
        "# Define a model with Batch Normalization\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, input_shape=(784,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dense(64),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and train the model\n",
        "model = build_model()\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=256, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "# Fold Batch Normalization into the preceding Dense layer\n",
        "def fold_batch_norm(model):\n",
        "    new_model = models.Sequential()\n",
        "    prev_layer_output_shape = None\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.BatchNormalization):\n",
        "            continue  # Skip BatchNorm layers\n",
        "        if isinstance(layer, layers.Dense):\n",
        "            # Extract weights and biases\n",
        "            weights, biases = layer.get_weights()\n",
        "            if prev_layer_output_shape:\n",
        "                # Incorporate BatchNorm parameters into Dense layer weights\n",
        "                gamma, beta = prev_layer_bn_params\n",
        "                mean, variance = prev_layer_bn_moments\n",
        "                scale = gamma / tf.sqrt(variance + 1e-5)\n",
        "                offset = beta - (gamma * mean) / tf.sqrt(variance + 1e-5)\n",
        "                weights = weights * scale\n",
        "                biases = biases - (offset * scale) + beta\n",
        "\n",
        "            # Add Dense layer to the new model\n",
        "            new_model.add(layers.Dense(layer.units,\n",
        "                                       activation=layer.activation,\n",
        "                                       kernel_initializer=layer.kernel_initializer,\n",
        "                                       bias_initializer=layer.bias_initializer,\n",
        "                                       input_shape=(prev_layer_output_shape,) if prev_layer_output_shape else None))\n",
        "            new_model.layers[-1].set_weights([weights, biases])\n",
        "\n",
        "        elif isinstance(layer, layers.ReLU):\n",
        "            new_model.add(layer)\n",
        "        elif isinstance(layer, layers.InputLayer):\n",
        "            new_model.add(layer)\n",
        "\n",
        "        # Save BatchNorm parameters from previous layer if applicable\n",
        "        if isinstance(layer, layers.BatchNormalization):\n",
        "            prev_layer_bn_params = [layer.gamma, layer.beta]\n",
        "            prev_layer_bn_moments = [layer.moving_mean, layer.moving_variance]\n",
        "\n",
        "        prev_layer_output_shape = layer.units if isinstance(layer, layers.Dense) else prev_layer_output_shape\n",
        "\n",
        "    return new_model\n",
        "\n",
        "# Fold the Batch Normalization layers\n",
        "new_model = fold_batch_norm(model)\n",
        "\n",
        "# Verify that the forward pass is consistent\n",
        "def verify_models(model1, model2, x_data):\n",
        "    preds1 = model1.predict(x_data)\n",
        "    preds2 = model2.predict(x_data)\n",
        "    return np.allclose(preds1, preds2)\n",
        "\n",
        "# Compare predictions from the original and new model\n",
        "is_same = verify_models(model, new_model, x_test[:10])\n",
        "print(f\"Forward pass is consistent: {is_same}\")\n",
        "\n",
        "# Plot loss values from history\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "wb6M71nFbQ6k",
        "outputId": "44994bed-d5db-458f-b7b6-28aa7b5c8b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 4s 9ms/step - loss: 0.3601 - accuracy: 0.9054 - val_loss: 0.1834 - val_accuracy: 0.9581\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1177 - accuracy: 0.9680 - val_loss: 0.1063 - val_accuracy: 0.9683\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0759 - accuracy: 0.9787 - val_loss: 0.0904 - val_accuracy: 0.9726\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0517 - accuracy: 0.9855 - val_loss: 0.0809 - val_accuracy: 0.9768\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0392 - accuracy: 0.9891 - val_loss: 0.0745 - val_accuracy: 0.9770\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.0782 - val_accuracy: 0.9754\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 0.0780 - val_accuracy: 0.9763\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0769 - val_accuracy: 0.9757\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.0813 - val_accuracy: 0.9749\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.0778 - val_accuracy: 0.9776\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.9776\n",
            "Test accuracy: 0.9775999784469604\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0df47f2f811d>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Fold the Batch Normalization layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Verify that the forward pass is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0df47f2f811d>\u001b[0m in \u001b[0;36mfold_batch_norm\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Add Dense layer to the new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             new_model.add(layers.Dense(layer.units,\n\u001b[0m\u001b[1;32m     58\u001b[0m                                        \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                        \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlayout_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_layout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Inject the layout parameter after the invocation of __init__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     ):\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    }
  ]
}